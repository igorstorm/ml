---
title: "Dever 2 Aprendizagem de máquinas"
output: html_document
---

#Problema 1

```{r}
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane, 
#    w[d+1] = -c is the negative offset parameter. 
#n: sample size

#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels

fakedata <- function(w, n){
  
  if(! require(MASS))
  {
    install.packages("MASS")
  }
  if(! require(mvtnorm))
  {
    install.packages("mvtnorm")
  }
  
  require(MASS)
  require(mvtnorm)
  
  # obtain dimension
  d <- length(w)-1
  
  # compute the offset vector and a Basis consisting of w and its nullspace
  offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
  Basis <- cbind(Null(w[1:d]), w[1:d])	 
  
  # Create samples, correct for offset, and extend
  # rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
  S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis) 
  S <- S + matrix(rep(offset,n),n,d,byrow=T)
  S <- cbind(S,1)
  
  # compute the class assignments
  y <- as.vector(sign(S %*% w))
  
  # add corrective factors to points that lie on the hyperplane.
  S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
  y = as.vector(sign(S %*% w))
  return(list(S=S, y=y))
  
} # end function fakedata
```

##Problema 1.1

### função que classifica a matriz dos dados em duas classes 1 e -1.

``` {r}
classify <- function(S,z) {
  y <- sign(S %*% z)
  y
  }
```

##Problema 1.2

### função que treina um vetor Z a partir de dados resultantes da função "fakedata" (matriz S e classe y) para classificar dados de teste futuramente.

``` {r}
perceptrain <- function(S,y){
  m <- ncol(S)
  n <- nrow(S)
  z <- rnorm(m)
  z_hist <- matrix(ncol=m,nrow=1)
  z_hist[1,] <- z
  
  for(k in 1:10000) {
    custo <- 0
    dc <- 0
    z_hist <- rbind(z_hist, matrix(ncol = m, nrow = 1))
    
    for (i in 1:n) {
      x_linha <- S[i,]
      
      if (sign(z %*% x_linha) != y[i]) {
      custo <- custo + abs(z %*% x_linha)
      dc <- dc + (-y[i]) * x_linha
       }
    }
    
    if(custo == 0){
      return(z_hist[-(k+1),])
    }
    
    z <- z - 1/k * dc
    z_hist[k+1,] <- z
    
    }
  return(k)
}
```

##Problema 1.3

### Cria-se um vetor Z aleatório de dimensão 3, cria-se uma matriz de dados falsos com a função fakedata, 100 observações, treina-se um vetor Z com a função perceptrain, resulta uma matriz cuja última linha é o perceptron, cria-se uma base de dados para teste e classifica essa nova base com a função "classify" com o perceptron, comparando com a classe "real" provinda desta base de dados criada.

```{r}
z <- rnorm(3) #Criando o vetor de 3 dimensões

treino <- fakedata(z,100) #matriz de dados para treinar o perceptron

perceptron <- perceptrain(treino$S,treino$y) #treinando um perceptron

teste <- fakedata(z,100) #matriz de dados para testar o perceptron

n <- nrow(perceptron)

n #numero de iterações

perceptron[n,]  #Vetor perceptron de 3 dimensões já treinado

classes_teste <- classify(teste$S, perceptron[n,]) #classificando a matriz de dados teste 

table(teste$y==classes_teste) #contabilizando as classificações "false"=má classificados "true"=corretamente classificados
```

### Vemos o resultado acima. Número de iterações para achar o perceptron, o perceptron e a contabilização de classificações certas.

##problema 1.4

```{r}
x <- teste$S[which(teste$y==1),1:2] #pontos da dimensão 1 e 2 que são classificados a direita

x1 <- teste$S[which(teste$y==-1),1:2] #pontos da dimensão 1 que são classificados a esquerda

erro <- treino$S[which(teste$y!=classes_teste),1:2] # pontos da dimensão 1 que estão mal classificados

plot(x, pch = c("+"), col = "blue", xlim = c(-4,4), ylim = c(-4,4), xlab = "Dimensão 1", ylab = "Dimensão 2", main = "Hyperplano classificador perceptron")

points(x1, pch = c("x"), col = "green")

points(erro,col="red")

legend("topleft", title = "Classificações", 
       c("1","-1","classificado errado"),fill = c("blue","green","red"),cex=.5,bty="n",inset=.02)

r <- perceptron[n,] #perceptron

m <- (1/(sqrt(r[1]^2 + r[2]^2))) * r #dividindo pela norma o perceptron

R <- Null(m[1:2])  #achando o vetor que é perpendicular ao vetor normal achado pelo perceptron

b <- R[2]/R[1] #inclinação do hyperplano

abline(a = -sign(m[2]) * m[3] * sqrt(b^2 + 1), b = b, col = 1)

```

### Segue explicação dos pontos na legenda, a reta é o classificador.

## problema 1.4

```{r}
hyperplanos <- function(T,treino) {
  n <- nrow(T)
  for (i in 1:n) {
    m <- (1/(sqrt(T[i,1]^2 + T[i,2]^2))) * T[i, ] #normalizando cada vetor Z
    r <- Null(m[1:2]) #vetor perpendicular ao vetor normal
    if (i < n) {
      b = r[2]/r[1] #inclinação da reta
      plot( 0, 0, cex = 0.0001, xlim = c(-4,4), ylim = c(-4,4), ann = F,
            xaxt = "n", yaxt = "n")
      abline(a = -sign(m[2]) * m[3] * sqrt(b^2 + 1), b = b)
      par(new = TRUE)
    }
    else {
      plot( 0, 0, cex = 0.0001, xlim = c(-4,4), ylim = c(-4,4), xlab = 
              "Dimensão 1", ylab = "Dimensão 2", main = "Evolução do vetor Z")
      b = r[2]/r[1] #inclinação da reta
      abline(a = -sign(m[2]) * m[3] * sqrt(b^2 + 1), b = r[2]/r[1],col="blue")
    }
  }
  points(treino$S[which(treino$y==1),1:2], pch = c("+"), col = "green")
  points(treino$S[which(treino$y==-1),1:2], pch = c("x"), col = "red")
  par(new = TRUE)
}

hyperplanos(perceptron,treino)
```

### Reta azul indica o último vetor Z correspondente ao perceptron, pontos (+) em verde significam os dados classificados como +1, pontos (x) em vermelho significam os dados classificados como -1, retas pretas são os vetores Z sendo ajustados.
